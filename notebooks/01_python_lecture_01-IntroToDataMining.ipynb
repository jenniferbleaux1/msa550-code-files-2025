{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"mount_file_id":"1q3GCibCX7MnWtDLb7pxyEdC6taAcSO2D","authorship_tag":"ABX9TyOakZCQPw+rxjbP/C9bd2FB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Lecture 01 — Introduction to Data Mining (Python)\n","\n","**Term:** Fall 2025  \n","\n","**Week/Topic:** Lecture 01 — Data Exploration & Linear Regression  \n","\n","**Instructor:** Dr. Bushaj  \n","\n","### What we'll cover\n","- Colab/Drive setup & data-path conventions\n","- Imports & aliases (`numpy`→`np`, `pandas`→`pd`, `matplotlib.pyplot`→`plt`, `sklearn`, `seaborn`, `dmba`)\n","- Load dataset: **West Roxbury Housing**\n","- Clean column names & access patterns (`loc`, `iloc`, dot notation)\n","- Quick EDA: shape, head, dtypes, describe, value counts\n","- Sampling: random, weighted oversampling, group-wise sampling\n","- Data quality checks: missing values & \"unseen\" missing values (e.g., `YR_BUILT == 0`)\n","- Handling missing data & types (impute, fill, convert to categorical)\n","- Visualization: distributions & correlation heatmap\n","- Scaling/normalizing features (StandardScaler, MinMaxScaler)\n","- Partitioning data (train/validation/test, stratified option)\n","- Linear Regression: fit, predict, coefficients\n","- Model evaluation: MSE, RMSE, R², residuals; `regressionSummary`\n","- Scoring validation data & predicting for new records\n","\n","---\n","\n","> We often import packages with short aliases for readability:\n",">\n","> ```python\n","> import <package> as <alias>\n","> ```\n","> Common aliases include `np` (NumPy), `pd` (pandas), and `plt` (matplotlib.pyplot).\n",">\n","> This notebook expects a data path appropriate to your environment (e.g., a Drive-mounted path in Colab).\n"],"metadata":{"id":"iItjTJ6yGZej"}},{"cell_type":"markdown","source":["## Import Needed Functionality"],"metadata":{"id":"Mf-RByvfGfeW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0gClK3aGGV5"},"outputs":[],"source":["# Import necessary libraries for data manipulation, modeling, and plotting\n","\n","# install dmba package\n","!pip install dmba\n","# import the utility function regressionSummary\n","from dmba import regressionSummary\n","import numpy as np\n","import pandas as pd\n","import sklearn\n","from sklearn.model_selection import train_test_split   # For splitting the dataset\n","from sklearn.metrics import r2_score,mean_squared_error    # For evaluating the model's performance\n","from sklearn.linear_model import LinearRegression  # For performing linear regression# Linear Regression model\n","import matplotlib.pyplot as plt     # For plotting actual vs predicted values\n","#The abbreviations pd, np, and sm are commonly used in the data science community.\n","#help(sklearn)"]},{"cell_type":"markdown","source":["## 1. Data Exploration in Python (Loading, View, and Summarize)"],"metadata":{"id":"eNgIvWf4Giys"}},{"cell_type":"markdown","source":["### Load Data"],"metadata":{"id":"_P9BE8paG6an"}},{"cell_type":"code","source":["#create a folder with all the data files there and get the path to that folder\n","my_drive_path = \"/content/drive/MyDrive/SUNY/Class Material/2024 Fall/MSA550A/Python Class Work/msa550-code-files/data/\""],"metadata":{"id":"__zjL5MuG7Xm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Dataset: West Roxbury Housing Data**\n","\n","\n","    'TOTAL VALUE': 'Total assessed value of the property (in dollars)',\n","    'TAX': 'Annual property tax (in dollars)',\n","    'LOT SQFT': 'Lot size in square feet',\n","    'YR BUILT': 'Year the property was built',\n","    'GROSS AREA': 'Gross area of the property (in square feet)',\n","    'LIVING AREA': 'Living area in square feet',\n","    'FLOORS': 'Number of floors in the property',\n","    'ROOMS': 'Total number of rooms in the property',\n","    'BEDROOMS': 'Number of bedrooms in the property',\n","    'FULL BATH': 'Number of full bathrooms',\n","    'HALF BATH': 'Number of half bathrooms',\n","    'KITCHEN': 'Indicates if the property has a kitchen (1 = yes, 0 = no)',\n","    'FIREPLACE': 'Number of fireplaces in the property',\n","    'REMODEL': 'Indicates whether the property has been remodeled'"],"metadata":{"id":"XI2IT8EXG86H"}},{"cell_type":"code","source":["# Load data int a Pandas Dataframe\n","housing_df = pd.read_csv(my_drive_path+'WestRoxbury.csv')\n","\n","# Show the shape (dimensions) of the DataFrame: (number of rows, number of columns)\n","housing_df.shape\n","\n","# Display the first 5 rows to get an initial look at the dataset\n","housing_df.head()  #show the 1st five rows\n","#print(housing_df)  #show all the data\n","\n","# In case you want to view all the rows at once, you can adjust the display option:\n","pd.set_option('display.max_rows', None)\n","#print(housing_df)\n","\n","\n","# Print the list of column names to see which variables are present in the dataset\n","print(housing_df.columns)  # print a list of variables\n","#it can be tricky to access these columns with spaces. We can edit all\n","housing_df['TOTAL VALUE '] #you need to add those spaces. The spaces also affect this housing_df.TOTAL VALUE --wont work\n","\n","# Rename columns: replace spaces with '_' housing_df = housing_df.rename(columns={'TOTAL VALUE ': 'TOTAL_VALUE'})\n","#housing_df = housing_df.rename(columns={'TOTAL VALUE ': 'TOTAL_VALUE'})\n","# this will change all the names of the columns according to this rule\n","housing_df.columns = [s.strip().replace(' ', '_')  for s in housing_df.columns]\n","\n","# Print the cleaned column names to verify the changes\n","print(housing_df.columns)\n","\n","\n","# Accessing subsets of the data\n","# Pandas provides two main methods to access rows in a DataFrame: `loc` and `iloc`.\n","# `loc`: Access rows by label (name or index)\n","# `iloc`: Access rows by integer positions (numerical index)\n","# Example: Let's show the first four rows using both methods.\n","\n","# Using `loc` to show rows 0 to 3 (inclusive)\n","housing_df.loc[0:3]\n","\n","# Using `iloc` to show rows 0 to 3 (iloc excludes the last index, so 4 is not included)\n","housing_df.iloc[0:4]\n","\n","\n","# There are different ways to access the first 10 values in a column (TOTAL_VALUE)\n","\n","# Option 1: Using the column name directly and `iloc` to limit rows\n","housing_df['TOTAL_VALUE'].iloc[0:10]\n","\n","# Option 2: Using `iloc` first to slice the rows, then selecting the column\n","housing_df.iloc[0:10]['TOTAL_VALUE']\n","\n","# Option 3: Using dot notation (only works if the column name has no spaces)\n","housing_df.iloc[0:10].TOTAL_VALUE\n","\n","\n","# Accessing specific rows and columns\n","# Show the fifth row of the first 10 columns in different ways:\n","housing_df.iloc[4][0:10]  # Method 1: returning a single row as a Series\n","housing_df.iloc[4, 0:10]  # Method 2: similar but simpler syntax\n","housing_df.iloc[4:5, 0:10]  # Method 3: using a slice returns a DataFrame (useful if you need a DataFrame, not Series)\n","\n","# Concatenating multiple columns (including non-consecutive columns)\n","# Using pd.concat to combine two separate slices of columns (axis=1 means we combine them side by side as columns)\n","pd.concat([housing_df.iloc[4:6, 0:2], housing_df.iloc[4:6, 4:6]], axis=1)\n","\n","\n","# Accessing a full column in various ways\n","housing_df.iloc[:, 0:1]  # Access first column using iloc\n","housing_df.TOTAL_VALUE  # Dot notation to access TOTAL_VALUE column\n","housing_df['TOTAL_VALUE'][0:10]  # Access the first 10 rows of the TOTAL_VALUE column\n","\n","# Basic descriptive statistics\n","\n","# Find and print the number of rows (length) of the TOTAL_VALUE column\n","print('Number of rows:', len(housing_df['TOTAL_VALUE']))\n","# Calculate and print the mean of the TOTAL_VALUE column\n","print('Mean of TOTAL_VALUE:', housing_df['TOTAL_VALUE'].mean())\n","\n","# Display a summary of descriptive statistics for all columns (mean, median, std, etc.)\n","housing_df.describe()"],"metadata":{"id":"P_PUbmW-G_Fk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sampling (undersampling or oversampling)"],"metadata":{"id":"PKfEuhP-zp0j"}},{"cell_type":"code","source":["# Select a random sample of 5 observations from the DataFrame\n","# This helps us to get a quick look at a subset of the data\n","print(\"Random sample of 5 observations:\")\n","print(housing_df.sample(5))\n","\n","# Oversampling\n","# The goal of oversampling is to address class imbalances or to emphasize certain observations in the data.\n","# Here, we are oversampling houses with more than 10 rooms to give them more importance.\n","\n","# Create a list of weights for each observation based on the number of rooms\n","# We assign a higher weight (0.9) to houses with more than 10 rooms and a lower weight (0.01) to others\n","# This effectively means that houses with more than 10 rooms are more likely to be included in the sample\n","weights = [0.9 if rooms > 10 else 0.01 for rooms in housing_df['ROOMS']]\n","\n","print(\"Weights for each observation:\")\n","print(weights)\n","\n","# Take a random sample of 5 observations, using the weights to influence the sampling process\n","# Observations with higher weights are more likely to be selected\n","print(\"Oversampled sample of 5 observations with weights:\")\n","print(housing_df.sample(5, weights=weights))\n","\n","\n","#Other Ways to sample:\n","\n","# Assuming we have a 'TARGET' (in this case I used ROOMS) column indicating class labels\n","# Split the data into training and test sets while maintaining the class distribution\n","train_df, test_df = train_test_split(housing_df, test_size=0.2, stratify=housing_df['ROOMS'])\n","\n","#note that this simpling is specified in the train_test_split method\n","\n","print(\"Training set:\")\n","print(train_df.head())\n","print(\"Test set:\")\n","print(test_df.head())\n","\n","#Group based sampling:\n","# Example: Sampling 2 observations from each group defined by the number of rooms\n","grouped_sample = housing_df.groupby('ROOMS').apply(lambda x: x.sample(2))\n","print(\"Sampled 2 observations from each group based on the number of rooms:\")\n","print(grouped_sample)"],"metadata":{"id":"ahAFIkrEzrvE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Reviewing Variables\n","\n","To review each variable's composition in a dataset, you generally want to examine:\n","\n","\n","\n","1.   Data Types: Understand what type of data each column contains.\n","2.   Unique Values: Explore the unique values for categorical variables\n","3.   Statistical Summary: For numerical variables, get summary statistics like mean, median, min, and max.\n","4.   Missing Values: Check for any missing or NaN values.\n","5.   Value Counts: For categorical variables, view the distribution of values.\n"],"metadata":{"id":"wfHbL3bmz2wL"}},{"cell_type":"code","source":["# 1. Review Data Types\n","print(\"Data types of each column:\")\n","print(housing_df.dtypes)\n","\n","# 2. Explore Unique Values\n","print(\"\\nUnique values for each column:\")\n","for column in housing_df.columns:\n","    print(f\"\\nColumn: {column}\")\n","    print(housing_df[column].unique())\n","\n","\n","# 3. Statistical Summary\n","print(\"\\nSummary statistics for numerical columns:\")\n","print(housing_df.describe())\n","\n","# 4. Check for Missing Values\n","print(\"\\nMissing values in each column:\")\n","print(housing_df.isnull().sum())\n","\n","# 5. Value Counts for Categorical Variables\n","print(\"\\nValue counts for categorical columns:\")\n","for column in housing_df.select_dtypes(include=['object']).columns:\n","    print(f\"\\nColumn: {column}\")\n","    print(housing_df[column].value_counts())"],"metadata":{"id":"d-vhIa9Nz5U9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**CAN YOU NOTICE ANY ISSUES ON THE RESULT ABOVE???**\n"],"metadata":{"id":"UvbPsWTZz97m"}},{"cell_type":"markdown","source":["1. Data Types\n","* REMODEL Column: The REMODEL column is of type object, which is expected since it contains categorical data. However, the presence of NaN values indicates that this column is not fully populated.\n","2. Unique Values\n","* REMODEL Column: Contains NaN, 'Recent', and 'Old'. This suggests that there are missing values in this column which need to be handled.\n","3. Summary Statistics\n","* YR_BUILT Column: There is a 0 value in the YR_BUILT column, which seems out of place as it should represent the year the house was built. This likely indicates an error or missing value.\n","* FLOORS Column: Contains values 1.0, 1.5, 2.0, 2.5, and 3.0. This is reasonable, but make sure that these values accurately reflect the number of floors in the dataset.\n","* HALF_BATH Column: Contains values from 0 to 3. The average (0.614) suggests that many houses have zero half baths, which could be verified for consistency.\n","\n","4. Missing Values\n","* REMODEL Column: This column has 4346 missing values out of 5802 total rows, which is a significant proportion (about 75%). This might indicate a need for imputation or a decision to exclude this column if it's not crucial.\n","5. Value Counts for Categorical Columns\n","* REMODEL Column: The value counts show that 'Recent' is much more common than 'Old', and the large number of missing values should be addressed."],"metadata":{"id":"RQuyPFVy0BWg"}},{"cell_type":"code","source":["#an unseen missing data!!!\n","\n","# Replace '0' in 'YR_BUILT' with NaN\n","housing_df['YR_BUILT'].replace(0, np.nan, inplace=True)\n","\n","print(housing_df.REMODEL.unique())\n","print(housing_df.YR_BUILT.unique())"],"metadata":{"id":"vUhw_zp00JRs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot distributions of numerical variables\n","import seaborn as sns\n","\n","numerical_cols = housing_df.select_dtypes(include=['float64', 'int64']).columns\n","for col in numerical_cols:\n","    plt.figure()\n","    sns.histplot(housing_df[col].dropna(), kde=True)\n","    plt.title(f'Distribution of {col}')\n","    plt.show()\n","\n","# Explore relationships between 'TOTAL_VALUE' and other numerical features\n","plt.figure(figsize=(12, 8))\n","sns.heatmap(housing_df[numerical_cols].corr(), annot=True, cmap='coolwarm')\n","plt.title('Correlation Matrix')\n","plt.show()\n"],"metadata":{"id":"Ergr1Lux0KZS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dealing with missing data\n"],"metadata":{"id":"qQuzPk_e0ccX"}},{"cell_type":"code","source":["# Check for NaN values in the entire DataFrame\n","nan_check = housing_df.isna().sum()\n","# Display the number of NaN values for each column\n","print(\"Number of NaN values in each column:\")\n","print(nan_check)"],"metadata":{"id":"dyhhqdbt0c8F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To illustrate missing data procedures, we first convert a few entries for  bedrooms to NA's.\n","# Then we impute these missing values using the median of the remaining values.\n","\n","missingRows = housing_df.sample(10).index\n","housing_df.loc[missingRows, 'BEDROOMS'] = np.nan\n","print('Number of rows with valid BEDROOMS values after setting to NAN: ', housing_df['BEDROOMS'].count())\n","\n","# remove rows with missing values\n","#reduced_df = housing_df.dropna()\n","#print('Number of rows after removing rows with missing values: ', len(reduced_df))\n","\n","\n","# replace the missing values using the median of the remaining values.\n","medianBedrooms = housing_df['BEDROOMS'].median()\n","housing_df.BEDROOMS = housing_df.BEDROOMS.fillna(value=medianBedrooms)\n","print('Number of rows with valid BEDROOMS values after filling NA values: ',housing_df['BEDROOMS'].count())\n","\n","housing_df['YR_BUILT'].fillna(housing_df['YR_BUILT'].median(), inplace=True)\n","\n","\n","housing_df['REMODEL'].fillna('None', inplace=True)\n","print(\"Missing values after handling:\\n\", housing_df.isnull().sum())\n"],"metadata":{"id":"U2AKFLJa0ehY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the current type of the 'REMODEL' column\n","print(type(housing_df.REMODEL))\n","\n","# Convert 'REMODEL' to a categorical variable\n","housing_df.REMODEL = housing_df.REMODEL.astype('category')\n","\n","# Display the categories of the 'REMODEL' column\n","print(housing_df.REMODEL.cat.categories)  # Shows the unique categories\n","\n","# Verify the type of the 'REMODEL' column after conversion\n","print(housing_df.REMODEL.dtype)  # Should print 'category'"],"metadata":{"id":"20-YmVEb0hi9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Normalizing Data"],"metadata":{"id":"G2VHzoFJ0jlG"}},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","df = housing_df.iloc[:, 0:3].copy()\n","\n","# StandardScaler rescales the data to have a mean of 0 and a standard deviation of 1.\n","# This is known as standardization. It assumes that the data follows a normal distribution\n","# and transforms the data such that it falls within a standard normal distribution (z-score) with values ranging\n","# approximately between -3 and +3 for most data points. The formula used is:\n","# z = (x - mean) / standard_deviation\n","# where 'x' is an individual value, 'mean' is the mean of the column, and 'standard_deviation' is the standard deviation of the column.\n","\n","\n","# pandas:\n","norm_df = (df - df.mean())/df.std()\n","# This manually standardizes the DataFrame by subtracting the mean and dividing by the standard deviation for each column.\n","\n","\n","# scikit-learn implementation:\n","scaler = StandardScaler()\n","norm_df = pd.DataFrame(scaler.fit_transform(df), index=df.index,\n","   columns=df.columns)\n","# The StandardScaler in scikit-learn automatically performs standardization. It computes the mean and standard deviation,\n","# then transforms the data accordingly. The result is a numpy array, which we convert back to a DataFrame with the original indices and column names.\n","\n","print(norm_df.head())\n","\n","\n","# Rescaling a data frame\n","#When MinMaxScaler is used the it is also known as Normalization and it transform all\n","#the values in range between (0 to 1) formula is x = [(value - min)/(Max- Min)]\n","\n","\n","# scikit-learn implementation:\n","scaler = MinMaxScaler()\n","norm_df = pd.DataFrame(scaler.fit_transform(df), index=df.index,\n","   columns=df.columns)\n","# The MinMaxScaler in scikit-learn performs normalization automatically. It scales the data based on the minimum and maximum values of each column.\n","# The result is a numpy array, which we convert back to a DataFrame with the original indices and column names.\n","\n","print(norm_df.head())"],"metadata":{"id":"Vlt9tfkC0j-T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Partitioning the data"],"metadata":{"id":"TufuazLF0m8n"}},{"cell_type":"code","source":["# set random_state for reproducibility\n","random_state = 1\n","\n","# Split data into training (60%) and validation (40%)\n","trainData, validData = train_test_split(housing_df, test_size=0.40, random_state=random_state)\n","print(f\"Training data: {len(trainData)} rows\")\n","print(f\"Validation data: {len(validData)} rows\")\n","\n","# Split data into training (50%), validation (30%), and test (20%)\n","trainData, temp = train_test_split(housing_df, test_size=0.50, random_state=random_state)\n","validData, testData = train_test_split(temp, test_size=0.40, random_state=random_state)\n","print(f\"Training data: {len(trainData)} rows\")\n","print(f\"Validation data: {len(validData)} rows\")\n","print(f\"Test data: {len(testData)} rows\")"],"metadata":{"id":"wkIqbktY0nRp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"syyrs89IM48S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## EXAMPLE - Linear Regression: West Roxbury Housing Data"],"metadata":{"id":"YTnMEDKi2nst"}},{"cell_type":"markdown","source":["### 1. Data Prep"],"metadata":{"id":"BmyD2lO42uHN"}},{"cell_type":"code","source":["# Load the dataset from a CSV file into a pandas DataFrame\n","housing_df = pd.read_csv(my_drive_path + 'WestRoxbury.csv')\n","\n","# Clean column names:\n","# - Strip leading/trailing whitespace\n","# - Convert to lowercase for consistency\n","# - Replace spaces with underscores for easier access\n","housing_df.columns = [s.strip().lower().replace(' ', '_') for s in housing_df.columns]\n","\n","# Check for missing values in the dataset before handling them\n","print(\"Missing values before handling:\\n\", housing_df.isnull().sum())\n","\n","# Handle missing values:\n","# Here we are filling the nan values in the Remodel with None\n","# Alternatively, we drop any rows with missing values.\n","#housing_df = housing_df.dropna()  # Drop rows with missing values\n","# OR you could use imputation methods (like filling with mean/median).\n","\n","\n","housing_df['remodel'].fillna('None', inplace=True)\n","print(\"Missing values after handling:\\n\", housing_df.isnull().sum())\n","\n","\n","# Create dummy variables for categorical features:\n","# This converts categorical variables into a format that can be provided to ML algorithms.\n","# `drop_first=True` avoids multicollinearity by dropping the first category.\n","housing_df = pd.get_dummies(housing_df, prefix_sep='_', drop_first=True)\n","\n","\n","# Display info about the changes\n","print(\"\\nDataset info after handling missing values:\")\n","housing_df.info()"],"metadata":{"id":"Jf1pvodd2vEN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Data Partitioning"],"metadata":{"id":"o-ZO_7OW3ORd"}},{"cell_type":"code","source":["# Define predictors and outcome:\n","# Exclude specified columns from predictors.\n","exclude_columns = ('total_value', 'tax')  # Columns we don't want to use as predictors\n","predictors = [col for col in housing_df.columns if col not in exclude_columns]  # List of predictor variables\n","outcome = 'total_value'  # The target variable we want to predict\n","\n","\n","# Create X and y\n","X = housing_df[predictors]\n","y = housing_df[outcome]\n","\n","print(X.head())\n","print(y.head())\n","\n","# Split the data into training and validation sets\n","train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\n","\n","# Print the shapes of the resulting datasets\n","print(\"Shape of training features:\", train_X.shape)\n","print(\"Shape of training target:\", train_y.shape)\n","print(\"Shape of validation features:\", valid_X.shape)\n","print(\"Shape of validation target:\", valid_y.shape)\n","\n","\n","# Visualize the distribution of the target variable in both sets\n","plt.figure(figsize=(12, 6))\n","\n","plt.subplot(1, 2, 1)\n","sns.histplot(train_y, kde=True)\n","plt.title(\"Distribution of Target in Training Set\")\n","plt.xlabel(outcome)\n","\n","plt.subplot(1, 2, 2)\n","sns.histplot(valid_y, kde=True)\n","plt.title(\"Distribution of Target in Validation Set\")\n","plt.xlabel(outcome)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Check for any data leakage\n","common_indices = set(train_X.index) & set(valid_X.index)\n","if common_indices:\n","    print(\"Warning: There are common indices in train and validation sets!\")\n","    print(\"Common indices:\", common_indices)\n","else:\n","    print(\"No data leakage detected between train and validation sets.\")\n","\n","# Optional: Stratified split for imbalanced datasets\n","# If the target variable is categorical and imbalanced, consider using stratified split\n","if y.dtype == 'object' or y.nunique() < 10:\n","    train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1, stratify=y)\n","    print(\"\\nUsing stratified split due to categorical or imbalanced target variable.\")"],"metadata":{"id":"jk76ET6R3Ovy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Fit Model and Make Predictions (Training Data)"],"metadata":{"id":"jUzfTPsJ4XCq"}},{"cell_type":"code","source":["#Initialize and fit the model\n","model = LinearRegression()\n","model.fit(train_X, train_y)\n","\n","# Make predictions on training and validation sets\n","train_pred = model.predict(train_X)\n","valid_pred = model.predict(valid_X)\n","\n","# Create DataFrames with results\n","train_results = pd.DataFrame({'Actual': train_y, 'Predicted': train_pred})\n","train_results['Residual'] = train_results['Actual'] - train_results['Predicted']\n","\n","valid_results = pd.DataFrame({'Actual': valid_y, 'Predicted': valid_pred})\n","valid_results['Residual'] = valid_results['Actual'] - valid_results['Predicted']\n","\n","# Display sample of predictions\n","print(\"Sample of training predictions:\")\n","print(train_results.head())\n","\n","print(\"\\nSample of validation predictions:\")\n","print(valid_results.head())"],"metadata":{"id":"D22ZZ-Re4XfS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Calculate and display performance metrics\n","def print_metrics(y_true, y_pred, set_name):\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    print(f\"\\n{set_name} set metrics:\")\n","    print(f\"Mean Squared Error: {mse:.2f}\")\n","    print(f\"Root Mean Squared Error: {rmse:.2f}\")\n","    print(f\"R-squared Score: {r2:.4f}\")\n","\n","print_metrics(train_y, train_pred, \"Training\")\n","print_metrics(valid_y, valid_pred, \"Validation\")\n","\n","# Visualize actual vs predicted values\n","plt.figure(figsize=(12, 5))\n","\n","plt.subplot(1, 2, 1)\n","plt.scatter(train_y, train_pred, alpha=0.5)\n","plt.plot([train_y.min(), train_y.max()], [train_y.min(), train_y.max()], 'r--', lw=2)\n","plt.xlabel(\"Actual Values\")\n","plt.ylabel(\"Predicted Values\")\n","plt.title(\"Actual vs Predicted (Training Set)\")\n","\n","plt.subplot(1, 2, 2)\n","plt.scatter(valid_y, valid_pred, alpha=0.5)\n","plt.plot([valid_y.min(), valid_y.max()], [valid_y.min(), valid_y.max()], 'r--', lw=2)\n","plt.xlabel(\"Actual Values\")\n","plt.ylabel(\"Predicted Values\")\n","plt.title(\"Actual vs Predicted (Validation Set)\")\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Visualize residuals\n","plt.figure(figsize=(12, 5))\n","\n","plt.subplot(1, 2, 1)\n","sns.histplot(train_results['Residual'], kde=True)\n","plt.title(\"Distribution of Residuals (Training Set)\")\n","plt.xlabel(\"Residual\")\n","\n","plt.subplot(1, 2, 2)\n","sns.histplot(valid_results['Residual'], kde=True)\n","plt.title(\"Distribution of Residuals (Validation Set)\")\n","plt.xlabel(\"Residual\")\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Display feature importance\n","feature_importance = pd.DataFrame({'Feature': train_X.columns, 'Coefficient': model.coef_})\n","feature_importance = feature_importance.sort_values('Coefficient', key=abs, ascending=False)\n","print(\"\\nTop 10 most important features:\")\n","print(feature_importance.head(10))\n","\n","# Visualize feature importance\n","plt.figure(figsize=(12, 6))\n","sns.barplot(x='Coefficient', y='Feature', data=feature_importance.head(15))\n","plt.title(\"Top 15 Feature Importances\")\n","plt.show()"],"metadata":{"id":"C1dWTsxr4teq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What can we constitute from the feature importance???\n","\n","* Structural features like **number of floors and bathrooms** have the strongest positive relationships with total value.\n","* **Recent remodeling** significantly increases a property's value.\n","* Some counterintuitive relationships **(like with kitchens and bedrooms)** might warrant further investigation. They could be due to interactions between variables or could be capturing some other underlying factors.\n","* The importance of recent remodeling over old remodeling shows that the age and condition of improvements matter."],"metadata":{"id":"u1-6FMGIrYDJ"}},{"cell_type":"markdown","source":["### 4. Scoring the validation data"],"metadata":{"id":"-OZbsuVBrbzy"}},{"cell_type":"code","source":["valid_pred = model.predict(valid_X)\n","valid_results = pd.DataFrame({\n","    'Actual': valid_y,\n","    'Predicted': valid_pred,\n","    'Residual': valid_y - valid_pred})\n","valid_results.head()"],"metadata":{"id":"rmD2lVkyranZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5. Assess Accuracy"],"metadata":{"id":"PMLFNiOhrfZk"}},{"cell_type":"code","source":["\n","\n","# training set\n","regressionSummary(train_results.Actual, train_results.Predicted)\n","\n","# validation set\n","regressionSummary(valid_results.Actual, valid_results.Predicted)\n"],"metadata":{"id":"2iGuHEXprhGl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6. Predict New Data"],"metadata":{"id":"j_LuwJK1rnFQ"}},{"cell_type":"code","source":["new_data = pd.DataFrame({\n","    'LOT_SQFT': [4200, 6444, 5035],\n","    'YR_BUILT': [1960, 1940, 1925],\n","    'GROSS_AREA': [2670, 2886, 3264],\n","    'LIVING_AREA': [1710, 1474, 1523],\n","    'FLOORS': [2.0, 1.5, 1.9],\n","    'ROOMS': [10, 6, 6],\n","    'BEDROOMS': [4, 3, 2],\n","    'FULL_BATH': [1, 1, 1],\n","    'HALF_BATH': [1, 1, 0],\n","    'KITCHEN': [1, 1, 1],\n","    'FIREPLACE': [1, 1, 0],\n","    'REMODEL_Old': [0, 0, 0],\n","    'REMODEL_Recent': [0, 0, 1],\n","})\n","\n","# Convert column names to lowercase\n","new_data.columns = new_data.columns.str.lower()\n","\n","print(new_data)\n","\n","# Make sure the model's feature names are also in lowercase\n","model.feature_names_in_ = [name.lower() for name in model.feature_names_in_]\n","\n","print('Predictions: ', model.predict(new_data))"],"metadata":{"id":"lgaRAtHZro-y"},"execution_count":null,"outputs":[]}]}